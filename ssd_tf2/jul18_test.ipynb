{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.lite.python import interpreter\n",
    "\n",
    "\n",
    "# Seed the Numpy\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLiteObjectDetector_BVA():\n",
    "    \n",
    "    def __init__(self, model_name=\"default_tflite_model_bva\"):\n",
    "        self.model_name = model_name\n",
    "\n",
    "\n",
    "\n",
    "    def read_classes(self, class_file_path):\n",
    "        \"\"\"\n",
    "        This function reads the coco.names file and create a list which has all the Class names\n",
    "        \"\"\"\n",
    "        f = open(class_file_path)\n",
    "        self.class_name_list = {}\n",
    "        for i, val in enumerate(f):\n",
    "            print(\"{} > {}\".format(i,val))\n",
    "            self.class_name_list.update({i: val.strip()})\n",
    "            \n",
    "        f.close()\n",
    "   \n",
    "        \n",
    "        # FOR DEBUGGING\n",
    "        print(\"\\n[INFO] Number of Classes : {}\".format(len(self.class_name_list)))\n",
    "\n",
    "        # Creating a Uniform Distribution of Colors for Overlay of the Classes\n",
    "        self.color_list = np.random.uniform(0, 255, size = (len(self.class_name_list),3))\n",
    "        #FOR DEBUGGING\n",
    "        print(\"[INFO] Number of Random Colors Generated : {}\".format(len(self.color_list)))\n",
    "\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"\n",
    "        This functions loads the tflite model\n",
    "        \"\"\"\n",
    "        self.model_path = os.path.abspath(model_path)\n",
    "\n",
    "        print(\"\\n[INFO] Loading the '{}' Model...\".format(self.model_name))\n",
    "\n",
    "        # Load the model and create the interpreter\n",
    "\n",
    "        IMG_WIDTH,IMG_HEIGHT = 300, 300\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=self.model_path)\n",
    "        self.interpreter.allocate_tensors()\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    def configure_interpreter(self):\n",
    "        \"\"\"\n",
    "        This function configures the interpreter \n",
    "        \"\"\"\n",
    "        # Get the Input and Output Details\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        print(\"input_details : \", self.input_details)\n",
    "        print(\"\\n\\n\")\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "        print(\"output_details : \", self.output_details)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "        self.input_index = self.input_details[0][\"index\"]\n",
    "        print(\"input_index : \", self.input_index)\n",
    "        self.output_index = self.output_details[0][\"index\"]\n",
    "        print(\"output_index : \", self.output_index)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "        self.input_tensor_shape = self.input_details[0][\"shape\"]\n",
    "        self.IMG_HEIGHT =  self.input_tensor_shape[1]\n",
    "        self.IMG_WIDTH = self.input_tensor_shape[2]\n",
    "        print(\"IMG_HEIGHT : \", self.IMG_HEIGHT)\n",
    "        print(\"IMG_WIDTH : \", self.IMG_WIDTH)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    \n",
    "    def inference(self, input_image_path, min_conf_threshold=0.5):\n",
    "        \"\"\" This function performance the inference on the image and returns the results.\n",
    "\n",
    "        Args:\n",
    "            input_image_path ([string]): Path to the Image for Inference\n",
    "            min_conf_threshold (float, optional): Confidence Threshold of Prediction. Defaults to 0.5.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "            overlay_image (image) : image with the overlay\n",
    "            FRAME_INFERENCE_DICT (dictionary) : dictionary of all detected classes and its associated info\n",
    "            all_detected_objects (list) : list of all detected classes in the images\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        input_image_path = os.path.abspath(input_image_path)\n",
    "        input_image_bgr = cv2.imread(input_image_path)\n",
    "        overlay_image = input_image_bgr.copy()\n",
    "\n",
    "        height, width, channel = input_image_bgr.shape \n",
    "\n",
    "        input_image_rgb = cv2.cvtColor(input_image_bgr,cv2.COLOR_BGR2RGB)\n",
    "        input_image_rgb_resized = cv2.resize(input_image_rgb,(self.IMG_WIDTH,self.IMG_HEIGHT))\n",
    "        \n",
    "        #input_image_tensor = np.expand_dims(input_image_rgb_resized, axis=0).astype(np.float32)\n",
    "        input_image_tensor = np.expand_dims(input_image_rgb_resized, axis=0).astype(np.uint8)\n",
    "        print(input_image_tensor.shape)\n",
    "\n",
    "        # Perform Inference\n",
    "        self.interpreter.set_tensor(self.input_index, input_image_tensor)\n",
    "        self.interpreter.invoke()\n",
    "\n",
    "        # Retrieve Results\n",
    "        #https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py\n",
    "\n",
    "        #Retrieve detection results\n",
    "        boxes = self.interpreter.get_tensor(self.output_details[0]['index'])[0] # Bounding box coordinates of detected objects\n",
    "        classes = self.interpreter.get_tensor(self.output_details[1]['index'])[0] # Class index of detected objects\n",
    "        scores = self.interpreter.get_tensor(self.output_details[2]['index'])[0] # Confidence of detected objects\n",
    "        num_det = self.interpreter.get_tensor(self.output_details[3]['index'])[0]\n",
    "\n",
    "        # FOR DEBUG\n",
    "        print (\"boxes : \", boxes)\n",
    "        print(\"classes : \", classes)\n",
    "        print(\"scores : \", scores)\n",
    "        print(\"num_detect : \", num_det)\n",
    "\n",
    "        # DETECTION AND OVERLAY CODE\n",
    "        # List to store all detections in an Image\n",
    "        all_detected_objects = []\n",
    "        all_object_instance_list = []\n",
    "\n",
    "        # Dictionary of the entire frame\n",
    "        FRAME_INFERENCE_DICT = {}\n",
    "        all_object_dict = {}\n",
    "        \n",
    "\n",
    "                \n",
    "        #Loop over all detections and draw detection box if confidence is above minimum threshold\n",
    "        for i in range(len(scores)):\n",
    "            if ((scores[i] > min_conf_threshold) and (scores[i] <= 1.0)):\n",
    "\n",
    "                # Get bounding box coordinates and draw box\n",
    "                # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
    "                \n",
    "                ith_box = boxes[i]\n",
    "\n",
    "                # Raw Values\n",
    "                y_min, x_min, y_max, x_max = ith_box\n",
    "\n",
    "            \n",
    "                #bounding_box - raw \n",
    "                raw_bounding_box = {}\n",
    "                raw_bounding_box['y_min'] = y_min \n",
    "                raw_bounding_box['x_min'] = x_min\n",
    "                raw_bounding_box['y_max'] = y_max \n",
    "                raw_bounding_box['x_max'] = x_max\n",
    "                \n",
    "                # ORIGINAL CODE\n",
    "                # ymin = int(max(1,(boxes[i][0] * imH)))\n",
    "                # xmin = int(max(1,(boxes[i][1] * imW)))\n",
    "                # ymax = int(min(imH,(boxes[i][2] * imH)))\n",
    "                # xmax = int(min(imW,(boxes[i][3] * imW)))\n",
    "                \n",
    "                \n",
    "                # Scaled Co - Ordinate for Overlay\n",
    "                ymin = int(max(1,(y_min * height)))\n",
    "                xmin = int(max(1,(x_min * width)))\n",
    "                ymax = int(min(height,(y_max * height)))\n",
    "                xmax = int(min(width,(x_max * width)))\n",
    "                \n",
    "                # scaled_bounding_box\n",
    "                scaled_bounding_box = {}\n",
    "                scaled_bounding_box['y_min'] = y_min \n",
    "                scaled_bounding_box['x_min'] = x_min\n",
    "                scaled_bounding_box['y_max'] = y_max \n",
    "                scaled_bounding_box['x_max'] = x_max\n",
    "\n",
    "                ith_class_index = int(classes[i])\n",
    "                ith_class_name = self.class_name_list[ith_class_index] # Look up object name from \"labels\" array using class index\n",
    "                ith_class_confidence = scores[i]\n",
    "                ith_class_color = self.color_list[int(classes[i])]\n",
    "                \n",
    "                #overlay_text = '%s: %d%%' % (ith_class_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
    "                overlay_text = \"{}: {}%\".format(ith_class_name, ith_class_confidence)\n",
    "                print(overlay_text)\n",
    "\n",
    "\n",
    "                # FOR DEBUG\n",
    "                print(i)\n",
    "                print(\"ith_box : \", ith_box)\n",
    "                print(\"ith_class_confidence :\", ith_class_confidence)\n",
    "                print(\"ith_class_index : \", ith_class_index)\n",
    "                print(\"ith_class_name : \", ith_class_name)\n",
    "                print(\"ith_class_color : \",ith_class_color)\n",
    "                \n",
    "                print(\"Overlay Text : {}\".format(overlay_text))\n",
    "        \n",
    "\n",
    "\n",
    "                labelSize, baseLine = cv2.getTextSize(overlay_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
    "                label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
    "                \n",
    "                cv2.rectangle(overlay_image, (xmin,ymin), (xmax,ymax), ith_class_color, 2)\n",
    "                \n",
    "                cv2.rectangle(overlay_image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
    "                cv2.putText(overlay_image, overlay_text, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
    "\n",
    "                \n",
    "                # This list will accumalate all the detected classes\n",
    "                all_detected_objects.append(ith_class_name)\n",
    "\n",
    "                # This will be a new dictionary for each detection\n",
    "                ith_detection_dict = {}\n",
    "                ith_object_dict = {}\n",
    "                ith_detection_dict['bounding_box'] = raw_bounding_box\n",
    "                ith_detection_dict['scaled_bounding_box'] = scaled_bounding_box \n",
    "                ith_detection_dict['confidence'] = ith_class_confidence\n",
    "\n",
    "                ith_object_dict[ith_class_name] = ith_detection_dict\n",
    "\n",
    "\n",
    "                all_object_instance_list.append(ith_object_dict.copy())\n",
    "\n",
    "            all_object_dict['height'] = height\n",
    "            all_object_dict['width'] = width\n",
    "            all_object_dict['channel'] = channel\n",
    "\n",
    "            #all_object_dict['objects'] = ith_object_dict\n",
    "            all_object_dict['objects'] = all_object_instance_list\n",
    "            \n",
    "            FRAME_INFERENCE_DICT = all_object_dict\n",
    "            \n",
    "            all_detected_objects = list(set(all_detected_objects))\n",
    "\n",
    "        return overlay_image, FRAME_INFERENCE_DICT, all_detected_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFLITE_CLASS_PATH = \"../SSD of Vineet/coco.names\"\n",
    "\n",
    "TFLITE_MODEL_NAME = \"tflite_dynamic_range_quantization\"\n",
    "TFLITE_MODEL_PATH = \"ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/model_quan.tflite\"\n",
    "# TFLITE_MODEL_PATH = \"data/pretrained_models/bva_tflite/dynamic_range_quantization_model.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FOLDER  = os.path.abspath('Car')\n",
    "DUMMY_FILE_PATH  = os.path.abspath(\"Car/03099.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path_list(image_dir_path):\n",
    "\n",
    "    all_file_paths = []\n",
    "\n",
    "    acceptable_image_extensions = ('jpg', 'png', 'bmp', 'jpeg')\n",
    "\n",
    "\n",
    "    test_folder_dir = os.path.abspath(image_dir_path)\n",
    "\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    for root, dirs, files in os.walk(test_folder_dir, topdown=True):  \n",
    "        for file_name in files:\n",
    "            # Skip if the files i .DS_Store. This is specific to Mac OS\n",
    "            if file_name == \".DS_Store\":\n",
    "                continue\n",
    "            if file_name.lower().endswith(acceptable_image_extensions):\n",
    "                ith_file_path = os.path.abspath(os.path.join(root, file_name))\n",
    "                # FOR DEBUG\n",
    "                #print(ith_file_path)\n",
    "                all_file_paths.append(ith_file_path)\n",
    "\n",
    "    return all_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NUM_OF_IMAGES :  360\n"
     ]
    }
   ],
   "source": [
    "all_file_paths = get_image_path_list(TEST_FOLDER)\n",
    "NUM_OF_IMAGES = len(all_file_paths)\n",
    "print(\"NUM_OF_IMAGES : \", NUM_OF_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_start_time = time.time()\n",
    "tflite_instance = TFLiteObjectDetector_BVA(TFLITE_MODEL_NAME)\n",
    "print(tflite_instance.model_name)\n",
    "tflite_instance.read_classes(TFLITE_CLASS_PATH)\n",
    "tflite_instance.load_model(TFLITE_MODEL_PATH)\n",
    "tflite_instance.configure_interpreter()\n",
    "model_load_time = time.time() - model_load_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TFLiteObjectDetector_BVA' object has no attribute 'IMG_WIDTH'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/livid/ML/ssd_tf2/jul18_test.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/livid/ML/ssd_tf2/jul18_test.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39m# Perform Inference on the Dummy File to overcome Cold Start\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/livid/ML/ssd_tf2/jul18_test.ipynb#ch0000010?line=1'>2</a>\u001b[0m overlay_image, FRAME_INFERENCE_DICT, all_detected_objects \u001b[39m=\u001b[39m tflite_instance\u001b[39m.\u001b[39;49minference(DUMMY_FILE_PATH)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/livid/ML/ssd_tf2/jul18_test.ipynb#ch0000010?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(FRAME_INFERENCE_DICT)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/livid/ML/ssd_tf2/jul18_test.ipynb#ch0000010?line=5'>6</a>\u001b[0m \u001b[39m# # TIME PROFILING\u001b[39;00m\n",
      "\u001b[1;32m/Users/livid/ML/ssd_tf2/jul18_test.ipynb Cell 2'\u001b[0m in \u001b[0;36mTFLiteObjectDetector_BVA.inference\u001b[0;34m(self, input_image_path, min_conf_threshold)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/livid/ML/ssd_tf2/jul18_test.ipynb#ch0000001?line=95'>96</a>\u001b[0m height, width, channel \u001b[39m=\u001b[39m input_image_bgr\u001b[39m.\u001b[39mshape \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/livid/ML/ssd_tf2/jul18_test.ipynb#ch0000001?line=97'>98</a>\u001b[0m input_image_rgb \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(input_image_bgr,cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/livid/ML/ssd_tf2/jul18_test.ipynb#ch0000001?line=98'>99</a>\u001b[0m input_image_rgb_resized \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(input_image_rgb,(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mIMG_WIDTH,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mIMG_HEIGHT))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/livid/ML/ssd_tf2/jul18_test.ipynb#ch0000001?line=100'>101</a>\u001b[0m \u001b[39m#input_image_tensor = np.expand_dims(input_image_rgb_resized, axis=0).astype(np.float32)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/livid/ML/ssd_tf2/jul18_test.ipynb#ch0000001?line=101'>102</a>\u001b[0m input_image_tensor \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(input_image_rgb_resized, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TFLiteObjectDetector_BVA' object has no attribute 'IMG_WIDTH'"
     ]
    }
   ],
   "source": [
    "# Perform Inference on the Dummy File to overcome Cold Start\n",
    "overlay_image, FRAME_INFERENCE_DICT, all_detected_objects = tflite_instance.inference(DUMMY_FILE_PATH)\n",
    "print(FRAME_INFERENCE_DICT)\n",
    "\n",
    "\n",
    "# # TIME PROFILING\n",
    "\n",
    "inference_start_time = time.time()\n",
    "\n",
    "# #FOR DEBUG\n",
    "# # for i in range(2):\n",
    "# #     ith_image_path = all_file_paths[i]\n",
    "\n",
    "for ith_image_path in all_file_paths:\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    print(ith_image_path)\n",
    "\n",
    "    # Perform Inference on the ith_image_path\n",
    "    # overlay_image, FRAME_INFERENCE_DICT, all_detected_objects = tflite_instance.inference(ith_image_path)\n",
    "    overlay_image, FRAME_INFERENCE_DICT, all_detected_objects = tflite_instance.inference(ith_image_path,0.4)\n",
    "    print(FRAME_INFERENCE_DICT)\n",
    "    print(all_detected_objects)\n",
    "\n",
    "\n",
    "\n",
    "inference_time = time.time() - inference_start_time \n",
    "\n",
    "FPS = (NUM_OF_IMAGES / inference_time)\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "print(\"                             MODEL TIME PROFILING \")\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "print()\n",
    "print(\"MODEL LOAD TIME :\", model_load_time)\n",
    "print()\n",
    "print(\"NUMBER OF IMAGES : \", NUM_OF_IMAGES)\n",
    "print()\n",
    "print(\"TOTAL INFERENCE TIME : \", inference_time)\n",
    "print()\n",
    "print(\"FPS : \", FPS)\n",
    "print(\"------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ebbf2368461c7902bd16a6ea0ae2191240560be71e14bc1c4c199780b7be9b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
